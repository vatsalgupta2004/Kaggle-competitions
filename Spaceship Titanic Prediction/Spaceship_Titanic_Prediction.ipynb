{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117976,"databundleVersionId":14129261,"sourceType":"competition"},{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# Advanced Spaceship Titanic Solution\n# ============================================\n\nimport os, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n!pip -q install lightgbm xgboost catboost\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# ============================================\n# 1) Load Data\n# ============================================\nBASE = \"/kaggle/input/spaceship-titanic\"\ntrain = pd.read_csv(f\"{BASE}/train.csv\")\ntest = pd.read_csv(f\"{BASE}/test.csv\")\n\ny = train[\"Transported\"].astype(int)\nX = train.drop(columns=[\"Transported\"])\ntest_ids = test[\"PassengerId\"].values\n\n# ============================================\n# 2) Advanced Feature Engineering\n# ============================================\ndef engineer_advanced(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    \n    # === Basic Parsing ===\n    cab = df[\"Cabin\"].astype(str).str.split(\"/\", expand=True)\n    df[\"Deck\"] = cab[0].replace(\"nan\", np.nan)\n    df[\"CabinNum\"] = pd.to_numeric(cab[1], errors=\"coerce\")\n    df[\"Side\"] = cab[2].replace(\"nan\", np.nan)\n    \n    df[\"Group\"] = df[\"PassengerId\"].str.split(\"_\", expand=True)[0]\n    df[\"PersonNum\"] = pd.to_numeric(df[\"PassengerId\"].str.split(\"_\", expand=True)[1], errors=\"coerce\")\n    df[\"Surname\"] = df[\"Name\"].astype(str).str.split(\" \", expand=True)[1]\n    df[\"FirstName\"] = df[\"Name\"].astype(str).str.split(\" \", expand=True)[0]\n    \n    # === Spending Features ===\n    spend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n    for c in spend_cols:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    \n    df[\"SpendTotal\"] = df[spend_cols].sum(axis=1, skipna=True)\n    df[\"SpendMean\"] = df[spend_cols].mean(axis=1, skipna=True)\n    df[\"SpendStd\"] = df[spend_cols].std(axis=1, skipna=True).fillna(0)\n    df[\"SpendMax\"] = df[spend_cols].max(axis=1, skipna=True)\n    df[\"SpendNonZero\"] = (df[spend_cols].fillna(0) > 0).sum(axis=1)\n    df[\"NoSpendFlag\"] = (df[\"SpendTotal\"].fillna(0) == 0).astype(int)\n    \n    # Spending ratios\n    for c in spend_cols:\n        df[f\"{c}_Ratio\"] = df[c] / (df[\"SpendTotal\"] + 1)\n    \n    # Log transforms for skewed spending\n    df[\"SpendTotal_Log\"] = np.log1p(df[\"SpendTotal\"])\n    \n    # === Group Features ===\n    df[\"GroupSize\"] = df.groupby(\"Group\")[\"PassengerId\"].transform(\"count\")\n    df[\"IsAlone\"] = (df[\"GroupSize\"] == 1).astype(int)\n    df[\"SpendPerGroup\"] = df[\"SpendTotal\"] / df[\"GroupSize\"]\n    \n    # Group aggregates\n    for col in [\"Age\", \"SpendTotal\", \"VIP\", \"CryoSleep\"] + spend_cols:\n        if col in df.columns:\n            df[f\"Group_{col}_Mean\"] = df.groupby(\"Group\")[col].transform(\"mean\")\n            df[f\"Group_{col}_Std\"] = df.groupby(\"Group\")[col].transform(\"std\").fillna(0)\n            df[f\"Group_{col}_Max\"] = df.groupby(\"Group\")[col].transform(\"max\")\n            df[f\"Group_{col}_Min\"] = df.groupby(\"Group\")[col].transform(\"min\")\n    \n    # === Surname Features ===\n    df[\"SurnameSize\"] = df.groupby(\"Surname\")[\"PassengerId\"].transform(\"count\").fillna(1)\n    df[\"Surname_Age_Mean\"] = df.groupby(\"Surname\")[\"Age\"].transform(\"mean\")\n    df[\"Surname_Spend_Mean\"] = df.groupby(\"Surname\")[\"SpendTotal\"].transform(\"mean\")\n    \n    # === CryoSleep Logic ===\n    df[\"CryoSpendContradiction\"] = (\n        df[\"CryoSleep\"].fillna(False).astype(bool) & (df[\"SpendTotal\"].fillna(0) > 0)\n    ).astype(int)\n    \n    # === Age Features ===\n    df[\"AgeBucket\"] = pd.cut(df[\"Age\"], bins=[-0.01, 12, 18, 25, 35, 50, 120], labels=False)\n    df[\"IsChild\"] = (df[\"Age\"] < 18).astype(int)\n    df[\"IsYoungAdult\"] = ((df[\"Age\"] >= 18) & (df[\"Age\"] < 30)).astype(int)\n    df[\"IsSenior\"] = (df[\"Age\"] >= 60).astype(int)\n    \n    # === Cabin Features ===\n    df[\"CabinNumBucket\"] = pd.qcut(df[\"CabinNum\"], q=20, duplicates=\"drop\", labels=False)\n    df[\"CabinNum_Mod10\"] = df[\"CabinNum\"] % 10\n    df[\"CabinNum_IsEven\"] = (df[\"CabinNum\"] % 2 == 0).astype(int)\n    \n    # === Interactions ===\n    df[\"Deck_Side\"] = df[\"Deck\"].astype(str) + \"_\" + df[\"Side\"].astype(str)\n    df[\"Planet_Dest\"] = df[\"HomePlanet\"].astype(str) + \"_\" + df[\"Destination\"].astype(str)\n    df[\"Deck_Planet\"] = df[\"Deck\"].astype(str) + \"_\" + df[\"HomePlanet\"].astype(str)\n    df[\"Age_Deck\"] = df[\"AgeBucket\"].astype(str) + \"_\" + df[\"Deck\"].astype(str)\n    df[\"VIP_Planet\"] = df[\"VIP\"].astype(str) + \"_\" + df[\"HomePlanet\"].astype(str)\n    df[\"Cryo_Planet\"] = df[\"CryoSleep\"].astype(str) + \"_\" + df[\"HomePlanet\"].astype(str)\n    \n    # === Destination-based Features ===\n    df[\"Dest_Age_Mean\"] = df.groupby(\"Destination\")[\"Age\"].transform(\"mean\")\n    df[\"Dest_Spend_Mean\"] = df.groupby(\"Destination\")[\"SpendTotal\"].transform(\"mean\")\n    df[\"Planet_Age_Mean\"] = df.groupby(\"HomePlanet\")[\"Age\"].transform(\"mean\")\n    df[\"Planet_Spend_Mean\"] = df.groupby(\"HomePlanet\")[\"SpendTotal\"].transform(\"mean\")\n    \n    # === Deck-based Features ===\n    df[\"Deck_Cryo_Rate\"] = df.groupby(\"Deck\")[\"CryoSleep\"].transform(\"mean\")\n    df[\"Deck_VIP_Rate\"] = df.groupby(\"Deck\")[\"VIP\"].transform(\"mean\")\n    df[\"Deck_Age_Mean\"] = df.groupby(\"Deck\")[\"Age\"].transform(\"mean\")\n    \n    # === Missing value indicators ===\n    for col in [\"Age\", \"CabinNum\", \"HomePlanet\", \"Destination\", \"CryoSleep\", \"VIP\"]:\n        df[f\"{col}_Missing\"] = df[col].isna().astype(int)\n    \n    return df.drop(columns=[\"Name\", \"Cabin\"], errors=\"ignore\")\n\n# Engineer features\nall_df = pd.concat([X, test], ignore_index=True)\nall_fe = engineer_advanced(all_df)\n\nX_fe = all_fe.iloc[:len(X)].copy()\nT_fe = all_fe.iloc[len(X):].copy()\n\n# ============================================\n# 3) Advanced Preprocessing\n# ============================================\n# Identify column types\nnum_cols = [c for c in X_fe.columns if X_fe[c].dtype in ['int64', 'float64'] \n            and c not in ['CryoSleep', 'VIP', 'IsAlone', 'PassengerId']]\nbool_cols = ['CryoSleep', 'VIP', 'IsAlone', 'IsChild', 'IsYoungAdult', 'IsSenior', \n             'NoSpendFlag', 'CryoSpendContradiction', 'CabinNum_IsEven']\ncat_cols = ['HomePlanet', 'Destination', 'Deck', 'Side', 'Deck_Side', 'Group', \n            'Surname', 'Planet_Dest', 'Deck_Planet', 'Age_Deck', 'VIP_Planet', 'Cryo_Planet']\n\n# Remove high-cardinality categoricals\ncat_cols = [c for c in cat_cols if c in X_fe.columns and X_fe[c].nunique() < 500]\nnum_cols = [c for c in num_cols if c in X_fe.columns]\nbool_cols = [c for c in bool_cols if c in X_fe.columns]\n\nX_fe[bool_cols] = X_fe[bool_cols].astype(\"float\")\nT_fe[bool_cols] = T_fe[bool_cols].astype(\"float\")\n\nfor c in [\"PassengerId\", \"FirstName\"]:\n    if c in X_fe.columns: X_fe = X_fe.drop(columns=[c])\n    if c in T_fe.columns: T_fe = T_fe.drop(columns=[c])\n\n# Pipelines with robust scaling\nnum_pipe = Pipeline([\n    (\"imp\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", RobustScaler())\n])\nboo_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\"))])\ncat_pipe = Pipeline([\n    (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, max_categories=50))\n])\n\nprep = ColumnTransformer(\n    transformers=[\n        (\"num\", num_pipe, num_cols),\n        (\"boo\", boo_pipe, bool_cols),\n        (\"cat\", cat_pipe, cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\nX_mat = prep.fit_transform(X_fe)\nT_mat = prep.transform(T_fe)\n\nprint(f\"Feature matrix shape: {X_mat.shape}\")\n\n# ============================================\n# 4) Advanced Model Configuration\n# ============================================\nSEED = 42\nN_FOLDS = 10  # More folds for better generalization\n\n# Optimized models\nlgbm = LGBMClassifier(\n    n_estimators=3000, learning_rate=0.015, num_leaves=48,\n    subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n    min_child_samples=20, random_state=SEED, n_jobs=-1, verbose=-1\n)\n\nxgb = XGBClassifier(\n    n_estimators=2500, learning_rate=0.015, max_depth=5,\n    subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.5,\n    min_child_weight=3, gamma=0.1,\n    objective=\"binary:logistic\", eval_metric=\"logloss\",\n    tree_method=\"hist\", random_state=SEED, n_jobs=-1\n)\n\ncb = CatBoostClassifier(\n    iterations=3500, learning_rate=0.015, depth=6, \n    l2_leaf_reg=8, bagging_temperature=0.2,\n    loss_function=\"Logloss\", eval_metric=\"Accuracy\",\n    random_seed=SEED, verbose=False, od_type=\"Iter\", od_wait=150\n)\n\nrf = RandomForestClassifier(\n    n_estimators=500, max_depth=15, min_samples_split=10,\n    min_samples_leaf=4, max_features='sqrt',\n    random_state=SEED, n_jobs=-1\n)\n\n# ============================================\n# 5) Stratified K-Fold CV with Stacking\n# ============================================\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\noof_lgb = np.zeros(len(X_mat))\noof_xgb = np.zeros(len(X_mat))\noof_cb = np.zeros(len(X_mat))\noof_rf = np.zeros(len(X_mat))\n\ntest_lgb = np.zeros(len(T_mat))\ntest_xgb = np.zeros(len(T_mat))\ntest_cb = np.zeros(len(T_mat))\ntest_rf = np.zeros(len(T_mat))\n\nprint(\"\\n=== Training Base Models ===\")\nfor fold, (tr, va) in enumerate(skf.split(X_mat, y), 1):\n    X_tr, X_va = X_mat[tr], X_mat[va]\n    y_tr, y_va = y.iloc[tr], y.iloc[va]\n    \n    # LightGBM\n    lgbm.fit(X_tr, y_tr)\n    oof_lgb[va] = lgbm.predict_proba(X_va)[:, 1]\n    test_lgb += lgbm.predict_proba(T_mat)[:, 1] / N_FOLDS\n    \n    # XGBoost\n    xgb.fit(X_tr, y_tr)\n    oof_xgb[va] = xgb.predict_proba(X_va)[:, 1]\n    test_xgb += xgb.predict_proba(T_mat)[:, 1] / N_FOLDS\n    \n    # CatBoost\n    cb.fit(X_tr, y_tr)\n    oof_cb[va] = cb.predict_proba(X_va)[:, 1]\n    test_cb += cb.predict_proba(T_mat)[:, 1] / N_FOLDS\n    \n    # Random Forest\n    rf.fit(X_tr, y_tr)\n    oof_rf[va] = rf.predict_proba(X_va)[:, 1]\n    test_rf += rf.predict_proba(T_mat)[:, 1] / N_FOLDS\n    \n    acc_lgb = accuracy_score(y_va, (oof_lgb[va] >= 0.5).astype(int))\n    acc_xgb = accuracy_score(y_va, (oof_xgb[va] >= 0.5).astype(int))\n    acc_cb = accuracy_score(y_va, (oof_cb[va] >= 0.5).astype(int))\n    acc_rf = accuracy_score(y_va, (oof_rf[va] >= 0.5).astype(int))\n    \n    print(f\"Fold {fold:2d} | LGBM: {acc_lgb:.4f} | XGB: {acc_xgb:.4f} | CB: {acc_cb:.4f} | RF: {acc_rf:.4f}\")\n\n# Base model OOF scores\nprint(f\"\\n=== Base Model OOF Scores ===\")\nprint(f\"LGBM: {accuracy_score(y, (oof_lgb >= 0.5).astype(int)):.4f}\")\nprint(f\"XGB:  {accuracy_score(y, (oof_xgb >= 0.5).astype(int)):.4f}\")\nprint(f\"CB:   {accuracy_score(y, (oof_cb >= 0.5).astype(int)):.4f}\")\nprint(f\"RF:   {accuracy_score(y, (oof_rf >= 0.5).astype(int)):.4f}\")\n\n# ============================================\n# 6) Meta-Model Stacking\n# ============================================\nmeta_features_train = np.column_stack([oof_lgb, oof_xgb, oof_cb, oof_rf])\nmeta_features_test = np.column_stack([test_lgb, test_xgb, test_cb, test_rf])\n\n# Train meta-learner\nmeta_model = LogisticRegression(C=0.1, random_state=SEED, max_iter=1000)\nmeta_model.fit(meta_features_train, y)\n\noof_meta = meta_model.predict_proba(meta_features_train)[:, 1]\ntest_meta = meta_model.predict_proba(meta_features_test)[:, 1]\n\nprint(f\"Meta-Model OOF: {accuracy_score(y, (oof_meta >= 0.5).astype(int)):.4f}\")\n\n# ============================================\n# 7) Ensemble with Threshold Optimization\n# ============================================\n# Weighted ensemble of meta-model and best base models\noof_ensemble = 0.6 * oof_meta + 0.25 * oof_cb + 0.15 * oof_lgb\ntest_ensemble = 0.6 * test_meta + 0.25 * test_cb + 0.15 * test_lgb\n\n# Find optimal threshold\nths = np.linspace(0.35, 0.65, 61)\nbest_t, best_acc = 0.5, 0.0\nfor t in ths:\n    acc = accuracy_score(y, (oof_ensemble >= t).astype(int))\n    if acc > best_acc:\n        best_t, best_acc = t, acc\n\nprint(f\"\\n=== Final Results ===\")\nprint(f\"Best OOF Accuracy: {best_acc*100:.2f}% @ threshold={best_t:.3f}\")\nprint(f\"AUC-ROC: {roc_auc_score(y, oof_ensemble):.4f}\")\n\n# ============================================\n# 8) Generate Submission\n# ============================================\ntest_pred = (test_ensemble >= best_t).astype(bool)\n\nout_path = \"/kaggle/working/submission.csv\"\npd.DataFrame({\n    \"PassengerId\": test_ids, \n    \"Transported\": test_pred\n}).to_csv(out_path, index=False)\n\nprint(f\"\\nSubmission saved to: {out_path}\")\nprint(f\"Predicted True: {test_pred.sum()} ({100*test_pred.mean():.1f}%)\")\nprint(f\"Predicted False: {(~test_pred).sum()} ({100*(~test_pred).mean():.1f}%)\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:45:39.942954Z","iopub.execute_input":"2025-10-24T07:45:39.943730Z","iopub.status.idle":"2025-10-24T07:56:10.341535Z","shell.execute_reply.started":"2025-10-24T07:45:39.943704Z","shell.execute_reply":"2025-10-24T07:56:10.340701Z"}},"outputs":[{"name":"stdout","text":"Feature matrix shape: (8693, 233)\n\n=== Training Base Models ===\nFold  1 | LGBM: 0.8092 | XGB: 0.8023 | CB: 0.8023 | RF: 0.7897\nFold  2 | LGBM: 0.8103 | XGB: 0.8276 | CB: 0.8333 | RF: 0.8218\nFold  3 | LGBM: 0.8184 | XGB: 0.8092 | CB: 0.8207 | RF: 0.8115\nFold  4 | LGBM: 0.7871 | XGB: 0.7837 | CB: 0.7906 | RF: 0.7710\nFold  5 | LGBM: 0.8170 | XGB: 0.8147 | CB: 0.8124 | RF: 0.8078\nFold  6 | LGBM: 0.7975 | XGB: 0.8124 | CB: 0.8101 | RF: 0.7952\nFold  7 | LGBM: 0.8090 | XGB: 0.8113 | CB: 0.8239 | RF: 0.8021\nFold  8 | LGBM: 0.8113 | XGB: 0.8124 | CB: 0.8205 | RF: 0.8021\nFold  9 | LGBM: 0.8021 | XGB: 0.8021 | CB: 0.8136 | RF: 0.7986\nFold 10 | LGBM: 0.7871 | XGB: 0.7940 | CB: 0.7952 | RF: 0.7940\n\n=== Base Model OOF Scores ===\nLGBM: 0.8049\nXGB:  0.8070\nCB:   0.8123\nRF:   0.7994\nMeta-Model OOF: 0.8116\n\n=== Final Results ===\nBest OOF Accuracy: 81.35% @ threshold=0.460\nAUC-ROC: 0.9046\n\nSubmission saved to: /kaggle/working/submission.csv\nPredicted True: 2259 (52.8%)\nPredicted False: 2018 (47.2%)\n","output_type":"stream"}],"execution_count":2}]}